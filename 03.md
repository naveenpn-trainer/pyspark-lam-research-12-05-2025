# Revision

* Two objects will be created (spark,sc)

* How to create RDD

  * sc.parallelize
  * sc.textFile

* RDD is collection of partitions

  ```
  rdd = sc.parallelize([1,2,3,4,,5])
  rdd.getNumPartitions()
  
  rdd.map
  	.filter
  	.collect
  	.reduce
  	.count
  	.first
  ```

* Cluster Manager (local)

* Two Objects

* Web UI

* RDD operations

  * Transformation
  * Actions

**Spark SQL**

* A module in the spark ecosystem for structured and semi-structured
* SQL, DataFrame, DataSet
* local, Mesos, YARN, Spark Scheduler, Kubernetes



**DataFrame**

```
dfr = spark.read
dfr.
	.csv -> DataFrame
	.json
	.parquet
	.load
	.jdbc
```

**Different parameters**

.csv(path, sep, inferSchema,header,schema, quote, mode)

.json(path, multiLine)

**Bad Records**

1. PERMISSIVE
2. DROPMALFORMED
3. FAILFAST

**DataFrame**

.printSchema()

.columns

.select

.filter

.where

.show

.limit

.write

.save

.saveAsTable



**FUnctions**

from pyspark.sql.functions import 

* col
* when
* lit
* upper, 