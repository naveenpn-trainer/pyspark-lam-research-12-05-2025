# Apache Spark

> APache SPark is a **In-memory ** cluster computing framework designed to handle a wide range of big data workloads

1. Batch Processing
2. Stream Processing
3. Graph Computation
4. Machine Learning
5. Data INtegration

## PySpark

> PySpark is a Python API for Apache Spark.

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf5H1PEeyyfFWugLtxRHYtSVqN1PWUweQnvAAMOfDXc8hZGMaoAm94iVBy-jRjY0WXUN74l7U16yMoRKJ2O7dMG7k0CJ9W8vZENOWivPiLBRSvmKnaj6h2drmFXOETuAzDkdW1dg_6BC8Xy4smek7qXTYof?key=_he-T4Jq934AhrSZa-Be-g)

* Py4J

## Spark Ecosystem

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXeQ8nwU6lyOwJNSrkw2IQnlD2b8SaJiRwpQE5OfUZndRas7Bc3WiVZMbS9drCGn4LIyJXZrbPIfhNWH0s0IcsfYLzInVWsW0xQk0Wj-6d5M29xFq0kFPu2ihdsPdxXe1cMeosRDcVwMVw-04X4LnYPoSjow?key=_he-T4Jq934AhrSZa-Be-g)

## Spark API's

1. Low Level API's  (RDD's)
2. High Level API's

## SPark Interactive Shell

1. Spark Shell (Scala)
2. PySpark Shell (Python)

## RDD's (Resilient Distributed Dataset)

> A fundamental data structured of Apache SPark

## RDD Creation

There are two ways to create an RDD

1. Create an RDD from Collection

   ```python
   L = list(range(1,101))
   rdd = sc.parallelize(L)
   ```

   

2. Create an RDD from external source

   ```python
   rdd = sc.textFile("c:/users.csv")
   ```

   

## RDD/DataFrame Operations

Once an RDD is created we can perform two types of operations

1. Transformations
2. Actions

![img](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf-h_iWxcGh_jqTFfULTQw-rOgcPye1Fn5R3o_lAiH9kgptYJazJMhE13seC2gBUoLiy-eD7KVOK-EKap8ERVQnTvqk7OultdVw_jEJiVgJEi-bWsuahlOmOT3qUAQ9fykmCiN-7RzM-sHCn6upsQJ_Y8Y7?key=Dxp7lTxgvspH2ig-I7LuEw)





